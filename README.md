# âœ… Task 5: Decision Trees and Random Forests

## ğŸ“Œ Done by: Krishna

---

## âœ… Objective:
In this task, I (Krishna) worked on Decision Tree and Random Forest models for regression using the Housing.csv dataset.

---

## âœ… What I did step by step:

1. ğŸ“¥ First I uploaded the dataset (Housing.csv) in Google Colab.

2. ğŸ§¹ I checked for missing values and did One-Hot Encoding for categorical columns.

3. ğŸ§ª I split the data into X (features) and y (target - price).

4. ğŸ“Š I did Train-Test Split (80% train, 20% test).

5. ğŸŒ³ I trained a **Decision Tree Regressor** and checked its Mean Squared Error (MSE).

6. ğŸ‘ I visualized the Decision Tree with limited depth (max_depth=3 for graph, max_depth=5 for reducing overfitting).

7. ğŸŒ² After that, I trained a **Random Forest Regressor** and compared its performance with Decision Tree.

8. â­ I plotted **Feature Importance** from Random Forest.

9. ğŸ” Finally, I did **5-Fold Cross Validation** for Random Forest to check model stability.

---

## âœ… Libraries I Used:
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn

---

## âœ… Results I observed:

- âœ… Random Forest gave better performance (lower MSE) than Decision Tree.
- âœ… Limiting tree depth helped reduce overfitting.
- âœ… Feature Importance graph showed which features affect price prediction.
- âœ… Cross-validation showed stable performance.

---

## âœ… Folder Structure (for GitHub):


